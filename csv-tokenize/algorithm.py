import pandas as pd
from konlpy.tag import Okt
from multiprocessing import Pool, cpu_count
import os
import time
from datetime import datetime


def load_korean_stopwords(filepath: str):
    with open(filepath, 'r', encoding='utf-8') as f:
        return set(line.strip() for line in f)


def tokenize_and_clean(text, stopwords, ignore_words, remove_stopwords):
    okt = Okt()
    tokens = okt.nouns(str(text))

    if remove_stopwords and stopwords:
        tokens = [token for token in tokens if token not in stopwords]

    if ignore_words:
        tokens = [token for token in tokens if token not in ignore_words]

    return " ".join(tokens)


def parallel_tokenize(texts, stopwords, ignore_words, remove_stopwords):
    # ë©€í‹°í”„ë¡œì„¸ì‹±ìš© ë˜í¼ í•¨ìˆ˜
    from functools import partial
    with Pool(processes=cpu_count()) as pool:
        func = partial(tokenize_and_clean, stopwords=stopwords, ignore_words=ignore_words, remove_stopwords=remove_stopwords)
        return pool.map(func, texts)


def generate_report(
    df: pd.DataFrame,
    text_column: str,
    new_column: str,
    ignore_words: list,
    remove_stopwords: bool,
    keep_tokenized_column_only: bool,
    input_filename: str,
    output_filename: str,
    elapsed_time: float,
    total_tokens: int,
    unique_tokens: int
) -> str:
    """
    í† í°í™” ì‘ì—… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    
    Parameters:
    - df (pd.DataFrame): ì²˜ë¦¬ëœ DataFrame
    - text_column (str): ì›ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
    - new_column (str): í† í°í™”ëœ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
    - ignore_words (list): ë¬´ì‹œí•  ë‹¨ì–´ ëª©ë¡
    - remove_stopwords (bool): ë¶ˆìš©ì–´ ì œê±° ì—¬ë¶€
    - keep_tokenized_column_only (bool): í† í°í™”ëœ ì»¬ëŸ¼ë§Œ ìœ ì§€ ì—¬ë¶€
    - input_filename (str): ì…ë ¥ íŒŒì¼ ê²½ë¡œ
    - output_filename (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    - elapsed_time (float): ì†Œìš” ì‹œê°„ (ì´ˆ)
    - total_tokens (int): ì´ í† í° ìˆ˜
    - unique_tokens (int): ê³ ìœ  í† í° ìˆ˜
    
    Returns:
    - str: ìƒì„±ëœ ë³´ê³ ì„œ ë‚´ìš© (markdown í˜•ì‹)
    """
    report = f"""# CSV í† í°í™” ì‘ì—… ë³´ê³ ì„œ

## 1. ì‘ì—… ê°œìš”
- **ì‘ì—… ìœ í˜•**: CSV í…ìŠ¤íŠ¸ í† í°í™”
- **ì‹¤í–‰ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ì†Œìš” ì‹œê°„**: {elapsed_time:.2f}ì´ˆ

## 2. ì…ë ¥ ë°ì´í„°
- **ì…ë ¥ íŒŒì¼**: {input_filename}
- **í–‰ ìˆ˜**: {len(df):,}í–‰
- **ì»¬ëŸ¼ ìˆ˜**: {len(df.columns)}ê°œ
- **ì»¬ëŸ¼ ëª©ë¡**: {', '.join(df.columns)}

## 3. í† í°í™” ì„¤ì •
- **ì›ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼**: {text_column}
- **í† í°í™”ëœ í…ìŠ¤íŠ¸ ì»¬ëŸ¼**: {new_column}
- **ë¶ˆìš©ì–´ ì œê±°**: {'ì˜ˆ' if remove_stopwords else 'ì•„ë‹ˆì˜¤'}
- **ë¬´ì‹œí•  ë‹¨ì–´ ìˆ˜**: {len(ignore_words) if ignore_words else 0}ê°œ
- **í† í°í™”ëœ ì»¬ëŸ¼ë§Œ ìœ ì§€**: {'ì˜ˆ' if keep_tokenized_column_only else 'ì•„ë‹ˆì˜¤'}
- **ì‚¬ìš©ëœ CPU ì½”ì–´ ìˆ˜**: {cpu_count()}ê°œ

## 4. ì²˜ë¦¬ ê²°ê³¼
- **ì¶œë ¥ íŒŒì¼**: {output_filename}
- **ì´ í† í° ìˆ˜**: {total_tokens:,}ê°œ
- **ê³ ìœ  í† í° ìˆ˜**: {unique_tokens:,}ê°œ
- **í‰ê·  í† í° ìˆ˜/í–‰**: {total_tokens/len(df):.1f}ê°œ

## 5. ì„±ëŠ¥ ì§€í‘œ
- **ì²˜ë¦¬ ì†ë„**: {len(df) / elapsed_time:.2f} í–‰/ì´ˆ
- **í† í°í™” ì†ë„**: {total_tokens / elapsed_time:.2f} í† í°/ì´ˆ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB

## 6. ì‘ì—… ìƒíƒœ
- **ìƒíƒœ**: ì„±ê³µ
- **ì²˜ë¦¬ ê²°ê³¼**: í…ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ í† í°í™”ë¨
"""
    return report


def solution(data: object, text_column: str, output_filename: str, new_column: str = 'tokenized_text', ignore_words: list = None, remove_stopwords: bool = True, keep_tokenized_column_only: bool = False) -> tuple:
    """
    ëŒ€ìš©ëŸ‰ CSVì˜ íŠ¹ì • í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ í•œêµ­ì–´ í† í°í™” + ë¶ˆìš©ì–´ ì œê±°í•˜ì—¬ ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ê³ ì„±ëŠ¥ ë²„ì „.
    
    Returns:
    - tuple: (ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ, ë³´ê³ ì„œ ë‚´ìš©)
    """
    start_time = time.time()
    
    # CSV ë¡œë“œ ìµœì í™”
    dataFile = pd.read_csv(data, low_memory=False)
    dataFile[text_column] = dataFile[text_column].fillna("")

    stopwords = set()
    if remove_stopwords:
        stopwords = load_korean_stopwords('stopwords-ko.txt')

    if ignore_words:
        ignore_words = set(ignore_words)

    # ë³‘ë ¬ í† í°í™” ì²˜ë¦¬
    texts = dataFile[text_column].tolist()
    logger = print  # í•„ìš” ì‹œ ë¡œê±°ë¡œ ëŒ€ì²´ ê°€ëŠ¥
    logger(f"ğŸ” Tokenizing {len(texts)} rows using {cpu_count()} cores...")

    tokenized_results = parallel_tokenize(texts, stopwords, ignore_words, remove_stopwords)
    dataFile[new_column] = tokenized_results

    if keep_tokenized_column_only:
        dataFile = dataFile[[new_column]]

    # CSV ì €ì¥
    dataFile.to_csv(output_filename, index=False, encoding='utf-8-sig')
    logger(f"âœ… Saved tokenized result to {output_filename}")
    
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # í† í° í†µê³„ ê³„ì‚°
    all_tokens = " ".join(tokenized_results).split()
    total_tokens = len(all_tokens)
    unique_tokens = len(set(all_tokens))
    
    # ë³´ê³ ì„œ ìƒì„±
    report = generate_report(
        df=dataFile,
        text_column=text_column,
        new_column=new_column,
        ignore_words=ignore_words,
        remove_stopwords=remove_stopwords,
        keep_tokenized_column_only=keep_tokenized_column_only,
        input_filename=data.name if hasattr(data, 'name') else str(data),
        output_filename=output_filename,
        elapsed_time=elapsed_time,
        total_tokens=total_tokens,
        unique_tokens=unique_tokens
    )
    
    return output_filename, report